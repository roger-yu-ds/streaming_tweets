{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4af75ac0",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook uses a pretrained sparknlp model to extract entities from tweets to create a word cloud. The word cloud will contain the entities in the last 60 seconds and update every 10 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656f8179",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08fe65a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:15:06.415274Z",
     "iopub.status.busy": "2021-06-12T10:15:06.414841Z",
     "iopub.status.idle": "2021-06-12T10:15:06.441355Z",
     "shell.execute_reply": "2021-06-12T10:15:06.439741Z",
     "shell.execute_reply.started": "2021-06-12T10:15:06.415225Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pyspark.sql import (\n",
    "    SparkSession,\n",
    "    functions as F\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StringType,\n",
    "    StructType,\n",
    "    StructField,\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    BooleanType\n",
    ")\n",
    "import sparknlp\n",
    "from sparknlp import Finisher\n",
    "from pyspark.ml import (\n",
    "    Pipeline\n",
    ")\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "from wordcloud import (\n",
    "    WordCloud, \n",
    "    STOPWORDS, \n",
    "    ImageColorGenerator\n",
    ")\n",
    "\n",
    "from src.producers.twitter_kafka_producer import TwitterStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5376807e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:15:06.444248Z",
     "iopub.status.busy": "2021-06-12T10:15:06.443801Z",
     "iopub.status.idle": "2021-06-12T10:15:06.468693Z",
     "shell.execute_reply": "2021-06-12T10:15:06.467444Z",
     "shell.execute_reply.started": "2021-06-12T10:15:06.444203Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparknlp.version(): 3.1.0\n",
      "spark.version: 3.1.1\n"
     ]
    }
   ],
   "source": [
    "spark = sparknlp.start()\n",
    "print(f'sparknlp.version(): {sparknlp.version()}')\n",
    "print(f'spark.version: {spark.version}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c7913-d42f-40e8-a27c-77e184a5a71e",
   "metadata": {},
   "source": [
    "# Set up directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c472fa5d-158f-46af-9a83-23239e35367f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:15:06.471607Z",
     "iopub.status.busy": "2021-06-12T10:15:06.471234Z",
     "iopub.status.idle": "2021-06-12T10:15:06.483497Z",
     "shell.execute_reply": "2021-06-12T10:15:06.481724Z",
     "shell.execute_reply.started": "2021-06-12T10:15:06.471520Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_dir = Path.cwd().parent\n",
    "models_dir = project_dir / 'models'\n",
    "pretrained_models_dir = models_dir / 'pretrained'\n",
    "data_dir = project_dir / 'data'\n",
    "raw_data_dir = data_dir / 'raw'\n",
    "processed_data_dir = data_dir / 'processed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ad51e7-16db-41c1-8009-cda59e96c622",
   "metadata": {},
   "source": [
    "# Load pretrained pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a59015a7-b435-41db-b53b-13fc138d499b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:16:24.062541Z",
     "iopub.status.busy": "2021-06-12T10:16:24.062084Z",
     "iopub.status.idle": "2021-06-12T10:18:26.882547Z",
     "shell.execute_reply": "2021-06-12T10:18:26.880843Z",
     "shell.execute_reply.started": "2021-06-12T10:16:24.062511Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recognize_entities_dl download started this may take some time.\n",
      "Approx size to download 160.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'recognize_entities_dl'\n",
    "pretrained_pipeline = PretrainedPipeline(name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffa3c2c6-2456-4389-8370-f0ac34001ede",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:18:26.885218Z",
     "iopub.status.busy": "2021-06-12T10:18:26.884844Z",
     "iopub.status.idle": "2021-06-12T10:18:26.913467Z",
     "shell.execute_reply": "2021-06-12T10:18:26.911907Z",
     "shell.execute_reply.started": "2021-06-12T10:18:26.885181Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A Finisher is required to extract the ArrayColumn `entities`\n",
    "finisher = Finisher().setInputCols(['entities'])\n",
    "pipeline = Pipeline().setStages([\n",
    "    pretrained_pipeline.model,\n",
    "    finisher\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00c56d4-6b2e-4980-8088-bfa6c74c0772",
   "metadata": {},
   "source": [
    "# Read stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cac1a35-40ca-4182-b071-3f6c306c4bb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:21:57.169769Z",
     "iopub.status.busy": "2021-06-12T10:21:57.169449Z",
     "iopub.status.idle": "2021-06-12T10:21:57.185594Z",
     "shell.execute_reply": "2021-06-12T10:21:57.184389Z",
     "shell.execute_reply.started": "2021-06-12T10:21:57.169740Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweet_schema = StructType([\n",
    "    StructField('created_at', StringType(), True),\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField('text', StringType(), True),\n",
    "    StructField('is_quote_status', BooleanType(), True),\n",
    "    StructField('in_reply_to_user_id', LongType(), True),\n",
    "    StructField('user', StructType([\n",
    "        StructField('id', LongType(), True),\n",
    "        StructField('followers_count', IntegerType(), True),\n",
    "        StructField('friends_count', IntegerType(), True),\n",
    "        StructField('created_at', StringType(), True)\n",
    "    ])),\n",
    "    StructField('extended_tweet', StructType([\n",
    "        StructField('full_text', StringType(), True)\n",
    "    ])),\n",
    "    StructField('retweeted_status', StructType([\n",
    "        StructField('id', LongType(), True)\n",
    "    ])),\n",
    "    StructField('retweet_count', IntegerType(), True),\n",
    "    StructField('favorite_count', IntegerType(), True),\n",
    "    StructField('quote_count', IntegerType(), True),\n",
    "    StructField('reply_count', IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b926884-217c-469c-aa65-44c2586706a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:21:57.648186Z",
     "iopub.status.busy": "2021-06-12T10:21:57.647794Z",
     "iopub.status.idle": "2021-06-12T10:21:57.671578Z",
     "shell.execute_reply": "2021-06-12T10:21:57.670380Z",
     "shell.execute_reply.started": "2021-06-12T10:21:57.648149Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stream_df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"subscribe\", \"twitterdata\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2178b148-c65b-4d3c-bde4-cef55a3b3c24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:21:58.228088Z",
     "iopub.status.busy": "2021-06-12T10:21:58.227724Z",
     "iopub.status.idle": "2021-06-12T10:21:58.353893Z",
     "shell.execute_reply": "2021-06-12T10:21:58.352610Z",
     "shell.execute_reply.started": "2021-06-12T10:21:58.228056Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_stream_df = (\n",
    "    stream_df\n",
    "    # Convert the key and value from binary to StringType\n",
    "    .withColumn('key', stream_df['key'].cast(StringType()))\n",
    "    .withColumn('value', stream_df['value'].cast(StringType()))\n",
    "    # Assign fields to JSON\n",
    "    .withColumn('value', F.from_json('value', tweet_schema))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea9b9e2a-521d-4e0e-885f-d666ca2471f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:21:58.839049Z",
     "iopub.status.busy": "2021-06-12T10:21:58.838694Z",
     "iopub.status.idle": "2021-06-12T10:21:58.847192Z",
     "shell.execute_reply": "2021-06-12T10:21:58.845387Z",
     "shell.execute_reply.started": "2021-06-12T10:21:58.839017Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- text: string (nullable = true)\n",
      " |    |-- is_quote_status: boolean (nullable = true)\n",
      " |    |-- in_reply_to_user_id: long (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- followers_count: integer (nullable = true)\n",
      " |    |    |-- friends_count: integer (nullable = true)\n",
      " |    |    |-- created_at: string (nullable = true)\n",
      " |    |-- extended_tweet: struct (nullable = true)\n",
      " |    |    |-- full_text: string (nullable = true)\n",
      " |    |-- retweeted_status: struct (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |-- retweet_count: integer (nullable = true)\n",
      " |    |-- favorite_count: integer (nullable = true)\n",
      " |    |-- quote_count: integer (nullable = true)\n",
      " |    |-- reply_count: integer (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbbeb060-4b75-4887-9a93-787da3fb19b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:22:05.063841Z",
     "iopub.status.busy": "2021-06-12T10:22:05.063490Z",
     "iopub.status.idle": "2021-06-12T10:22:05.120286Z",
     "shell.execute_reply": "2021-06-12T10:22:05.117594Z",
     "shell.execute_reply.started": "2021-06-12T10:22:05.063814Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweet_stream_df = (\n",
    "    json_stream_df\n",
    "    .select('timestamp',\n",
    "            'value.created_at',\n",
    "            'value.text',\n",
    "            'value.extended_tweet.full_text')\n",
    "    .withWatermark(\"timestamp\", \"1 minutes\")\n",
    "#     .filter(F.col('timestamp') > datetime.now() - timedelta(seconds=60)) # This doesn't seem to restrict the data to the last 60 seconds because the count keeps on growing.\n",
    "#     .groupBy(F.window(json_stream_df.timestamp, window_duration, slide_duration))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e8b6dd2-2760-4b5b-b805-c06f17339f3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:22:05.714030Z",
     "iopub.status.busy": "2021-06-12T10:22:05.713573Z",
     "iopub.status.idle": "2021-06-12T10:22:05.723757Z",
     "shell.execute_reply": "2021-06-12T10:22:05.721417Z",
     "shell.execute_reply.started": "2021-06-12T10:22:05.713986Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- full_text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweet_stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d81481e0-16a1-475c-a3c4-e61a8b42521c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:22:09.485516Z",
     "iopub.status.busy": "2021-06-12T10:22:09.485160Z",
     "iopub.status.idle": "2021-06-12T10:22:10.167289Z",
     "shell.execute_reply": "2021-06-12T10:22:10.162554Z",
     "shell.execute_reply.started": "2021-06-12T10:22:09.485472Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweet_stream = (\n",
    "    tweet_stream_df\n",
    "    .writeStream\n",
    "    .format('memory')\n",
    "    .queryName('tweet_view')\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15f95f2a-d896-4f35-ab22-303269166fee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T11:19:50.781305Z",
     "iopub.status.busy": "2021-06-08T11:19:50.780850Z",
     "iopub.status.idle": "2021-06-08T11:19:50.922808Z",
     "shell.execute_reply": "2021-06-08T11:19:50.921690Z",
     "shell.execute_reply.started": "2021-06-08T11:19:50.781270Z"
    },
    "tags": []
   },
   "source": [
    "tweet_stream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d373fbe4-d729-4329-bd9b-ff7b9d4e0d09",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stream to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8dea4326-0b2a-4aac-b520-8aebd9c63965",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:22:13.173022Z",
     "iopub.status.busy": "2021-06-12T10:22:13.172606Z",
     "iopub.status.idle": "2021-06-12T10:22:13.180878Z",
     "shell.execute_reply": "2021-06-12T10:22:13.178997Z",
     "shell.execute_reply.started": "2021-06-12T10:22:13.172981Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "parquet_path = processed_data_dir / 'ner_parquet'\n",
    "checkpoint_path = processed_data_dir / 'ner_checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c551140-8724-4ff6-a406-44e6e5344222",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:22:13.718825Z",
     "iopub.status.busy": "2021-06-12T10:22:13.718426Z",
     "iopub.status.idle": "2021-06-12T10:22:13.838077Z",
     "shell.execute_reply": "2021-06-12T10:22:13.836225Z",
     "shell.execute_reply.started": "2021-06-12T10:22:13.718782Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o470.start.\n: java.io.IOException: mkdir of /data/processed/ner_parquet/_spark_metadata failed\n\tat org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:1280)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:183)\n\tat org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:212)\n\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:804)\n\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:800)\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n\tat org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:807)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.mkdirs(CheckpointFileManager.scala:309)\n\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.<init>(HDFSMetadataLog.scala:64)\n\tat org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.<init>(CompactibleFileStreamLog.scala:47)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSinkLog.<init>(FileStreamSinkLog.scala:86)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink.<init>(FileStreamSink.scala:141)\n\tat org.apache.spark.sql.execution.datasources.DataSource.createSink(DataSource.scala:330)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.createV1Sink(DataStreamWriter.scala:484)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:453)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:301)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-451c25f202a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m parquet_stream = (\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtweet_stream_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparquet_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_posix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o470.start.\n: java.io.IOException: mkdir of /data/processed/ner_parquet/_spark_metadata failed\n\tat org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:1280)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:183)\n\tat org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:212)\n\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:804)\n\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:800)\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n\tat org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:807)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.mkdirs(CheckpointFileManager.scala:309)\n\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.<init>(HDFSMetadataLog.scala:64)\n\tat org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.<init>(CompactibleFileStreamLog.scala:47)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSinkLog.<init>(FileStreamSinkLog.scala:86)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink.<init>(FileStreamSink.scala:141)\n\tat org.apache.spark.sql.execution.datasources.DataSource.createSink(DataSource.scala:330)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.createV1Sink(DataStreamWriter.scala:484)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:453)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:301)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "parquet_stream = (\n",
    "    tweet_stream_df\n",
    "    .writeStream\n",
    "    .option('path', parquet_path.as_posix())\n",
    "    .outputMode('append')\n",
    "    .option('checkpointLocation', checkpoint_path.as_posix())\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "405fff36-0f26-4615-8c74-ea1cb9c2e17c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:07:35.634062Z",
     "iopub.status.busy": "2021-06-12T10:07:35.633566Z",
     "iopub.status.idle": "2021-06-12T10:07:35.679056Z",
     "shell.execute_reply": "2021-06-12T10:07:35.677084Z",
     "shell.execute_reply.started": "2021-06-12T10:07:35.634007Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "parquet_stream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dd9310-7f8f-48ff-b28b-085838319412",
   "metadata": {},
   "source": [
    "# Create word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1557ffec-a064-4ba4-b1dd-359af94cb547",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:22:29.890529Z",
     "iopub.status.busy": "2021-06-12T10:22:29.890098Z",
     "iopub.status.idle": "2021-06-12T10:22:33.320116Z",
     "shell.execute_reply": "2021-06-12T10:22:33.317895Z",
     "shell.execute_reply.started": "2021-06-12T10:22:29.890482Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "We need at least 1 word to plot a word cloud, got 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-03d24321ee3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpdf_entities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinished_entities\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mwordcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordcloud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             raise ValueError(\"We need at least 1 word to plot a word cloud, \"\n\u001b[0m\u001b[1;32m    404\u001b[0m                              \"got %d.\" % len(frequencies))\n\u001b[1;32m    405\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: We need at least 1 word to plot a word cloud, got 0."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "while True:\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "      CASE WHEN ISNULL(full_text) THEN text \n",
    "           WHEN CHAR_LENGTH(text) > CHAR_LENGTH(full_text) THEN text ELSE full_text\n",
    "      END as text\n",
    "    FROM tweet_view\n",
    "    WHERE timestamp > (CURRENT_TIMESTAMP() - INTERVAL 60 seconds)\n",
    "    \"\"\"\n",
    "    tweet_df = spark.sql(query)\n",
    "    clear_output(wait=True)\n",
    "#     display(tweet_df.show(20))\n",
    "\n",
    "    # Extract entities\n",
    "    df_entities = pipeline.fit(tweet_df).transform(tweet_df)\n",
    "    \n",
    "    # Create word cloud: it takes about 10-15 seconds to process ~1100 tweets (60 seconds' worth of Tweets)\n",
    "    # into an NER word cloud.\n",
    "    # Most of the time goes into transforming into pandas dataframe\n",
    "    # It would probably be faster to use spark to create the Counter, but don't know how.\n",
    "    pdf_entities = df_entities.select('finished_entities').toPandas()\n",
    "    \n",
    "    # This line transforms a column of lists of entities into a Counter\n",
    "    c = Counter(chain.from_iterable(x for x in pdf_entities.finished_entities if len(x) > 0))\n",
    "    plt.figure(figsize=(15, 8), facecolor='k')\n",
    "    wordcloud = WordCloud(width=1200, height=600).generate_from_frequencies(c)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()\n",
    "    print(f'Number of tweets: {tweet_df.count()}')\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
