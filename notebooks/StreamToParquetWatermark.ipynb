{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Note: This is very similar to the StreamToParquet.ipynb, except in this one we will add an additional group column  which will be used to categories the tweets based on the subjects we are tracking, see twitter_kafka_producer.py.\n",
    "\n",
    "Import necassary libraries:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType, BooleanType\n",
    "from IPython.display import display, clear_output\n",
    "import time"
   ]
  },
  {
   "source": [
    "Setup connection and subscribed to the twitterdata topic:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open spark session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('kafka') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Subscribed to twitterdata topic\n",
    "stream_df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"subscribe\", \"twitterdata\") \\\n",
    "  .load()"
   ]
  },
  {
   "source": [
    "Convert the value column to string, then convert the JSON string to StructType *(online including the columns we are interested in)*:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_stream_df = stream_df.withColumn(\"value\", stream_df[\"value\"].cast(StringType()))\n",
    "tweet_schema = StructType([\n",
    "    StructField('created_at', StringType(), True),\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField('text', StringType(), True),\n",
    "    StructField('is_quote_status', BooleanType(), True),\n",
    "    StructField('in_reply_to_user_id', LongType(), True),\n",
    "    StructField('user', StructType([\n",
    "        StructField('id', LongType(), True),\n",
    "        StructField('followers_count', IntegerType(), True),\n",
    "        StructField('friends_count', IntegerType(), True),\n",
    "        StructField('created_at', StringType(), True)\n",
    "    ])),\n",
    "    StructField('extended_tweet', StructType([\n",
    "        StructField('full_text', StringType(), True)\n",
    "    ])),\n",
    "    StructField('retweeted_status', StructType([\n",
    "        StructField('id', LongType(), True),\n",
    "        StructField('extended_tweet', StructType([\n",
    "            StructField('full_text', StringType(), True)\n",
    "        ]))\n",
    "    ])),\n",
    "    StructField('retweet_count', IntegerType(), True),\n",
    "    StructField('favorite_count', IntegerType(), True),\n",
    "    StructField('quote_count', IntegerType(), True),\n",
    "    StructField('reply_count', IntegerType(), True)\n",
    "])\n",
    "struct_stream_df = string_stream_df.withColumn(\"value\", F.from_json(\"value\", tweet_schema))"
   ]
  },
  {
   "source": [
    "The text column will not always contain the relevant data for example quotes and retweets will usually only contains a truncated version of the orginal tweet and therfore may not contain the keyword we are tracking, for this reason to generate the *category* column we will concatinate the *text* and *full_text's* columns from extended_tweet & retweet_status then categorise based on keyword found in the new *combined_text* column."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pattern(value):\n",
    "    return f'(?i)({value})'\n",
    "\n",
    "twitter_flat_df = struct_stream_df.select(\n",
    "    'timestamp'\n",
    "    , 'value.text'\n",
    "    , F.col('value.retweeted_status.extended_tweet.full_text').alias('retweet_extended_full_text')\n",
    "    , F.col('value.extended_tweet.full_text').alias('extended_full_text')\n",
    ").withColumn(\n",
    "    'combined_text'\n",
    "    , F.concat_ws(' ', F.col('text'), F.col('retweet_extended_full_text'), F.col('extended_full_text'))\n",
    ").withColumn(\n",
    "    'category'\n",
    "    , F.when(F.col('combined_text').rlike(get_pattern('cryptocurrency')), 'cryptocurrency') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('crypto')), 'crypto') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('binance')), 'binance') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('coinbase')), 'coinbase') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('coinmarketcap')), 'coinmarketcap') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('musk')), 'musk') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('memecoin')), 'memecoin') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('shitcoin')), 'shitcoin') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('moon')), 'moon') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('hodl')), 'hodl') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('fud')), 'fud') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('bitcoin')), 'bitcoin') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('btc')), 'btc') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('ethereum')), 'ethereum') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('ether')), 'ether') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('gwei')), 'gwei') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('vitalik buterin')), 'vitalik buterin') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('gavin wood')), 'gavin wood') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('erc20')), 'erc20') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('dogecoin')), 'dogecoin') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('doge')), 'doge') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('billy markus')), 'billy markus') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('jackson palmer')), 'jackson palmer') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('pancakeswap')), 'pancakeswap') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('cake')), 'cake') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('swap')), 'swap') \\\n",
    "        .when(F.col('combined_text').rlike(get_pattern('eth')), 'eth')\n",
    "    )\n"
   ]
  },
  {
   "source": [
    "Quick sanity check make sure things are being categorised:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_flat_stream = twitter_flat_df.writeStream.format('memory').queryName('twitter_flat').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+--------------------+--------+\n|           timestamp|       combined_text|category|\n+--------------------+--------------------+--------+\n|2021-06-08 10:25:...|RT @CardanoFeed: ...|ethereum|\n|2021-06-08 10:25:...|RT @markjburns88:...|  crypto|\n|2021-06-08 10:25:...|My lovely buyer w...|  crypto|\n|2021-06-08 10:25:...|RT @Crypt0kyuubi:...|    hodl|\n|2021-06-08 10:25:...|RT @Deeeerin: - R...| bitcoin|\n|2021-06-08 10:25:...|Dogecoin Legacy s...|dogecoin|\n|2021-06-08 10:25:...|@hans_emerson @Cr...|    moon|\n|2021-06-08 10:25:...|        To the moon!|    moon|\n|2021-06-08 10:25:...|RT @toolzbeib: Th...|    moon|\n|2021-06-08 10:25:...|RT @hippiechikmar...|    moon|\n|2021-06-08 10:25:...|RT @blockchainrem...| bitcoin|\n|2021-06-08 10:25:...|RT @angry_coin: â€œ...|  crypto|\n|2021-06-08 10:25:...|@HukAleksandra ht...| bitcoin|\n|2021-06-08 10:25:...|RT @mskvsk: 22,55...| bitcoin|\n|2021-06-08 10:25:...|RT @NewWorld_Orde...| bitcoin|\n|2021-06-08 10:25:...|@SPYJared nobody ...|     fud|\n|2021-06-08 10:25:...|RT @FOXNFT2021: A...|    doge|\n|2021-06-08 10:25:...|RT @defi_sunio: ?...| binance|\n|2021-06-08 10:25:...|RT @KishuTriad: ?...| binance|\n|2021-06-08 10:25:...|RT @DimitarHadjie...| bitcoin|\n+--------------------+--------------------+--------+\nonly showing top 20 rows\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "None"
     },
     "metadata": {}
    }
   ],
   "source": [
    "clear_output(wait=True)\n",
    "display(spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        t.timestamp\n",
    "        , t.combined_text\n",
    "        , t.category \n",
    "    FROM twitter_flat t\n",
    "    \"\"\"\n",
    ").show(20))\n",
    "time.sleep(1)"
   ]
  },
  {
   "source": [
    "Looks good, stop the stream:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_flat_stream.stop()"
   ]
  },
  {
   "source": [
    "Here we are defining a windowed aggregation query to which will calculate the number of tweets coming in for each category in a 60 seconds time windows, updating every 10 seconds and with a late threshold of 15 seconds:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_duration = '60 seconds'\n",
    "slide_duration = '30 seconds'\n",
    "late_threshold = '15 seconds'\n",
    "\n",
    "window_df = twitter_flat_df.withWatermark(\n",
    "    'timestamp'\n",
    "    , late_threshold\n",
    "    ).groupby(\n",
    "        F.window(twitter_flat_df.timestamp, window_duration, slide_duration)\n",
    "        , twitter_flat_df.category\n",
    "    ).count()"
   ]
  },
  {
   "source": [
    "To sink this to parquet we need to do it in batches using the foreachBatch callback method, this will be executed once the time window has finshed, we can then use sink the batch of aggregations to parquet.\n",
    "\n",
    "Ran into a number of issues trying to get his to work, because its a windowed aggregated streaming data frame spark want let you just write it to parquet after a fair amount of googling I came across the following article which demonstrated persisting aggregated stream to a database table: https://docs.databricks.com/spark/latest/structured-streaming/examples.html\n",
    "\n",
    "Some of the issues encountered:\n",
    " - AnalysisException: Data source parquet does not support Complete output mode.\n",
    " - overwrite output mode no supported"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_batch(df, epochId):\n",
    "    df.write.parquet('data/twitter_windowed_sink.parquet', mode='overwrite')\n",
    "\n",
    "flat_stream_out = window_df \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(write_batch) \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "source": [
    "Let that run for a bit and then stop the streams:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_stream.stop()\n",
    "flat_stream_out.stop()"
   ]
  },
  {
   "source": [
    "Now lets take a quick look at the data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+--------------+-----+\n|              window|      category|count|\n+--------------------+--------------+-----+\n|{2021-06-06 10:51...|cryptocurrency|    2|\n|{2021-06-06 09:26...|           eth|    1|\n|{2021-06-06 10:24...|       binance|    3|\n|{2021-06-06 08:19...|cryptocurrency|    2|\n|{2021-06-06 07:46...|          hodl|    2|\n|{2021-06-06 09:20...|       bitcoin|    3|\n|{2021-06-06 09:45...|           btc|    2|\n|{2021-06-06 07:17...|          doge|    3|\n|{2021-06-06 08:10...|       binance|    6|\n|{2021-06-06 09:59...|          hodl|    2|\n|{2021-06-06 09:45...|           eth|    2|\n|{2021-06-06 08:36...|         ether|    1|\n|{2021-06-06 10:40...|       bitcoin|    3|\n|{2021-06-06 07:26...|cryptocurrency|    7|\n|{2021-06-06 09:59...|          swap|    1|\n|{2021-06-06 07:44...|          null|   14|\n|{2021-06-06 09:10...|        crypto|   11|\n|{2021-06-06 10:02...|           eth|    1|\n|{2021-06-06 07:03...|cryptocurrency|    2|\n|{2021-06-06 07:13...|      ethereum|    1|\n+--------------------+--------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('data/twitter_windowed_sink.parquet')\n",
    "df.show()"
   ]
  },
  {
   "source": [
    "Close the spark session:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ]
}